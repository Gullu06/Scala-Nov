{"cells":[{"cell_type":"code","execution_count":1,"id":"bf293bca","metadata":{},"outputs":[{"data":{"text/plain":["Intitializing Scala interpreter ..."]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Spark Web UI available at http://cluster-bd33-m:8088/proxy/application_1732719982311_0008\n","SparkContext available as 'sc' (version = 3.1.3, master = yarn, app id = application_1732719982311_0008)\n","SparkSession available as 'spark'\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["import org.apache.spark.{SparkConf, SparkContext}\n","import org.apache.spark.sql.functions._\n"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.{SparkConf, SparkContext}\n","import org.apache.spark.sql.functions._"]},{"cell_type":"code","execution_count":2,"id":"f3829973","metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+-------+------+-------------------+\n","|userId|movieId|rating|          timestamp|\n","+------+-------+------+-------------------+\n","|     1|      2|   3.5|2005-04-02 23:53:47|\n","|     1|     29|   3.5|2005-04-02 23:31:16|\n","|     1|     32|   3.5|2005-04-02 23:33:39|\n","|     1|     47|   3.5|2005-04-02 23:32:07|\n","|     1|     50|   3.5|2005-04-02 23:29:40|\n","+------+-------+------+-------------------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["ratingsPath: String = gs://priyanshi-spark-bucket-2/rating.csv\n","ratingsDF: org.apache.spark.sql.DataFrame = [userId: string, movieId: string ... 2 more fields]\n"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["// Load rating.csv from GCS\n","val ratingsPath = s\"gs://priyanshi-spark-bucket-2/rating.csv\"\n","val ratingsDF = spark.read.option(\"header\", \"true\").csv(ratingsPath)\n","\n","ratingsDF.show(5)"]},{"cell_type":"code","execution_count":3,"id":"84148d60","metadata":{},"outputs":[{"data":{"text/plain":["validRatingsDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userId: string, movieId: string ... 2 more fields]\n"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["// Filter out invalid records\n","val validRatingsDF = ratingsDF.filter(col(\"userId\").isNotNull && col(\"userId\").isNotNull && col(\"rating\").isNotNull)"]},{"cell_type":"code","execution_count":4,"id":"69aecb8b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(1,(2,3.5))\n","(1,(29,3.5))\n","(1,(32,3.5))\n","(1,(47,3.5))\n","(1,(50,3.5))\n"]},{"data":{"text/plain":["ratingsRDD: org.apache.spark.rdd.RDD[(String, (String, Double))] = MapPartitionsRDD[22] at map at <console>:29\n"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["// Convert DataFrame to RDD\n","val ratingsRDD = validRatingsDF.limit(100000).rdd.map(row => {\n","  val userId = row.getAs[String](\"userId\")\n","  val movieId = row.getAs[String](\"movieId\")\n","  val rating = row.getAs[String](\"rating\").toDouble\n","  (userId, (movieId, rating))  // (userId, (movieId, rating))\n","})\n","\n","ratingsRDD.take(5).foreach(println)"]},{"cell_type":"code","execution_count":5,"id":"5e323e24","metadata":{},"outputs":[{"data":{"text/plain":["groupedByUserRDD: org.apache.spark.rdd.RDD[(String, Iterable[(String, Double)])] = ShuffledRDD[23] at groupByKey at <console>:29\n"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["// Group ratings by userId\n","val groupedByUserRDD = ratingsRDD.groupByKey()"]},{"cell_type":"code","execution_count":6,"id":"c4aef99d","metadata":{},"outputs":[{"data":{"text/plain":["import org.apache.spark.rdd.RDD\n","import org.apache.hadoop.fs.{FileSystem, Path}\n","import java.io.{BufferedWriter, OutputStreamWriter}\n","first10Users: Array[(String, Iterable[(String, Double)])] = Array((273,CompactBuffer((50,4.0), (104,4.0), (247,4.0), (296,3.0), (305,2.0), (356,3.0), (535,5.0), (593,3.0), (608,4.0), (762,1.0), (904,3.0), (912,4.0), (919,3.0), (1078,3.0), (1193,4.0), (1197,2.0), (1203,3.0), (1204,4.0), (1206,4.0), (1213,3.0), (1221,4.0), (1230,5.0), (1233,3.0), (1247,3.0), (1270,3.0), (1300,4.0), (1617,2.0), (1635,4.0), (1734,4.0), (2289,4.0), (2351,4.0), (2590,4.0), (2692,3.0), (2858,4.0), (2920,4.0), (2978,3.0), (3019,3.0), (3185,3.0), (3198,4.0), (3298,2.0), (3317,3.0), (3408,4.0), (3468,3.0), (3481,4.0), (3538,3.0), (3556,3.0), (3567,4.0), (3594,4.0), (3751,2.0), (3753,1...\n"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.rdd.RDD\n","import org.apache.hadoop.fs.{FileSystem, Path}\n","import java.io.{BufferedWriter, OutputStreamWriter}\n","\n","val first10Users = groupedByUserRDD.take(10)\n","first10Users.foreach { case (userId, ratingsList) =>\n","  val userFolderPath = s\"hdfs:///user/priyanshi/user-data/${userId}/ratings.csv\"\n","  val path = new Path(userFolderPath)\n","  \n","  val fs = FileSystem.get(new java.net.URI(\"hdfs:///\"), new org.apache.hadoop.conf.Configuration())\n","  \n","  if (!fs.exists(path.getParent)) {\n","    fs.mkdirs(path.getParent)\n","  }\n","\n","  val ratingsText = ratingsList.map { case (movieId, rating) =>\n","    s\"${movieId}, ${rating}\"\n","  }.mkString(\"\\n\")\n","\n","  val outputStream = fs.create(path)\n","  val writer = new BufferedWriter(new OutputStreamWriter(outputStream))\n","\n","  writer.write(ratingsText)\n","\n","  writer.close()\n","  outputStream.close()\n","}"]},{"cell_type":"code","execution_count":10,"id":"abcdb325","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"6ea4c6ea","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"3d2f8994","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"spylon-kernel","language":"scala","name":"spylon-kernel"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","help_links":[{"text":"MetaKernel Magics","url":"https://metakernel.readthedocs.io/en/latest/source/README.html"}],"mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"0.4.1"}},"nbformat":4,"nbformat_minor":5}