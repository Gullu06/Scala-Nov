{"cells":[{"cell_type":"code","execution_count":null,"id":"fd4cafea","metadata":{},"outputs":[{"data":{"text/plain":["Intitializing Scala interpreter ..."]},"metadata":{},"output_type":"display_data"}],"source":["import org.apache.spark.sql.{SparkSession, functions => F}\n","import org.apache.spark.sql.types._\n","\n","// Step 2: Define schema for the ratings.csv file\n","val schema = StructType(Array(\n","  StructField(\"userId\", IntegerType, true),\n","  StructField(\"movieId\", IntegerType, true),\n","  StructField(\"rating\", DoubleType, true),\n","  StructField(\"timestamp\", LongType, true)\n","))"]},{"cell_type":"code","execution_count":null,"id":"1f5f2a11","metadata":{},"outputs":[],"source":["// Step 3: Load rating.csv as DataFrame from GCP Cloud Storage\n","val ratingsDF = spark.read\n","  .option(\"header\", \"true\")\n","  .schema(schema)\n","  .csv(\"gs://priyanshi-spark-bucket-2/rating.csv\")"]},{"cell_type":"code","execution_count":null,"id":"7e5b638e","metadata":{},"outputs":[],"source":["// Step 4: Transformation - Add year column by converting timestamp\n","val ratingsWithYearDF = ratingsDF.withColumn(\"year\", F.year(F.from_unixtime(F.col(\"timestamp\"))))\n","\n","val ratingsWithYearDF1 = ratingsWithYearDF.limit(1000)"]},{"cell_type":"code","execution_count":null,"id":"ac6daecd","metadata":{},"outputs":[],"source":["// Step 5: Convert DataFrame to RDD for partitioning by year\n","val ratingsByYearRDD = ratingsWithYearDF1.rdd.map(row => (row.getAs[Int](\"year\"), row))"]},{"cell_type":"code","execution_count":null,"id":"8ae5f8e4","metadata":{},"outputs":[],"source":["import org.apache.spark.sql.Row\n","\n","ratingsByYearRDD.groupByKey().foreach {\n","  case (year, records) =>\n","    // Convert records (Iterable[Row]) to an RDD[Row]\n","    val yearRDD = spark.sparkContext.parallelize(records.toSeq)\n","\n","    // Create DataFrame for the specific year\n","    val yearDF = spark.createDataFrame(yearRDD, ratingsWithYearDF.schema)\n","\n","    // Save the DataFrame as a Parquet file\n","    yearDF.write\n","      .mode(\"overwrite\")\n","      .parquet(s\"hdfs:///ratings/$year/rating.parquet\")\n","}"]},{"cell_type":"code","execution_count":null,"id":"e17a4f1e","metadata":{},"outputs":[],"source":["// Step 7: Verification (Optional)\n","// Load and count records for a specific year to ensure correctness\n","val year = 2020\n","val specificYearDF = spark.read.parquet(s\"hdfs:///ratings/$year/rating.parquet\")\n","println(s\"Record count for year $year: ${specificYearDF.count()}\")"]},{"cell_type":"code","execution_count":null,"id":"ff73e8da","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"37bbf1ab","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"spylon-kernel","language":"scala","name":"spylon-kernel"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","help_links":[{"text":"MetaKernel Magics","url":"https://metakernel.readthedocs.io/en/latest/source/README.html"}],"mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"0.4.1"}},"nbformat":4,"nbformat_minor":5}